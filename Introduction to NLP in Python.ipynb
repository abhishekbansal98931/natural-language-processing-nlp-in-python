{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expressions & Word Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regex are strings with a special syntax\n",
    "#Allows us to match patterns in other strings\n",
    "#Applications of regex: Find all web links in a document, parse email addresses, remove / replace unwanted characters etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#re.match(pattern, string)\n",
    "re.match('abc', 'abcdef')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 2), match='hi'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#special patterns\n",
    "word_regex = '\\w+'\n",
    "re.match(word_regex, 'hi there!') #matches first word in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common regex patterns (there are 100s of them and these are just a few common ones)\n",
    "# \\w+  word (\\w will just pull individual characters and \\w+ will pull word)\n",
    "# \\d   digit\n",
    "# \\s   space\n",
    "# .*   wildcard (matches any letter or symbol)\n",
    "# + or * greedy match (matches repeats of sigle letter or whole patterns)\n",
    "# \\S   not space\n",
    "# []   create a group of characters eg. [a-z] lowercase group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python re module\n",
    "#re: module\n",
    "#split: split a string on regex\n",
    "#findall: find all patterns in a string\n",
    "#search: search for a pattern\n",
    "#match: match an entire string or substring based on a pattern\n",
    "#sub: substitutes pattern with string Eg: re.sub(r\"\\?\", \" \", \"abc?\") this substitutes question mark with space\n",
    "\n",
    "#pass pattern first and then string\n",
    "#may return an iterator, string or match object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Split', 'on', 'spaces.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\s+', 'Split on spaces.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let', 's', 'wrtie', 'RegEx']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"\\w+\", \"Let's wrtie RegEx!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It is important to prefix regex patterns with r to ensure that patterns are interpreted correctly\n",
    "#specially escape sequences in strings\n",
    "#Eg: \"\\n\" in Python is used to indicate a new line but if we use r prefix, it will be interpreted as raw string \n",
    "#that is charater \"\\\" followed by character \"n\" and not as a new line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Let's write RegEx\",\n",
       " \"  Won't that be fun\",\n",
       " '  I sure think so',\n",
       " '  Can you find 4 sentences',\n",
       " '  Or [perhaps, all] 19 words',\n",
       " '']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or [perhaps, all] 19 words?\"\n",
    "\n",
    "#Split on Sentence endings\n",
    "re.split(r\"[.?!]\", my_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let', 'RegEx', 'Won', 'Can', 'Or']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find all capitalised words\n",
    "re.findall(r\"[A-Z]\\w+\", my_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Let's\",\n",
       " 'write',\n",
       " 'RegEx!',\n",
       " \"Won't\",\n",
       " 'that',\n",
       " 'be',\n",
       " 'fun?',\n",
       " 'I',\n",
       " 'sure',\n",
       " 'think',\n",
       " 'so.',\n",
       " 'Can',\n",
       " 'you',\n",
       " 'find',\n",
       " '4',\n",
       " 'sentences?',\n",
       " 'Or',\n",
       " 'perhaps,',\n",
       " 'all',\n",
       " '19',\n",
       " 'words?']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split string on spaces\n",
    "re.split(r\"\\s+\", my_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4', '19']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"\\d+\", my_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(2, 4), match='cd'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Difference between re.search() and re.match()\n",
    "#when the pattern that we are looking for is present in the beginning of the string then both will give identical results\n",
    "#Eg:\n",
    "re.match('abc','abcde') #will give result as 'abc'\n",
    "re.search('abc','abcde') #will give result as 'abc'\n",
    "\n",
    "#match tries to match a string from the beginning and search will go through the entire string\n",
    "#Eg:\n",
    "re.match('cd','abcde') #will not give any result\n",
    "re.search('cd','abcde') #will give result as 'cd'\n",
    "\n",
    "#use match when you want to look for pattern specifically at the beginning of the string\n",
    "#and use search when you want to look for pattern anywhere in the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(66, 70), match='find'>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Search for 1st occurence of \"find\"\n",
    "text1 = re.search(r\"find\", my_string)\n",
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 70\n"
     ]
    }
   ],
   "source": [
    "#Start and end indexes\n",
    "print(text1.start(), text1.end())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(88, 102), match='[perhaps, all]'>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find first text in square brackets\n",
    "re.search(r\"\\[.*]\", my_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 9), match='Abhishek:'>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find the script notation\n",
    "new_string = 'Abhishek: Hey, what are you doing?'\n",
    "re.match(r\".*:\", new_string)\n",
    "#OR\n",
    "re.match(r\"[a-zA-Z0-9\\s]+:\", new_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turning a string or document into tokens (smaller chunks)\n",
    "#One step in preparing a text for NLP\n",
    "#Many different theories and rules\n",
    "#You can create your own rules using regular expressions\n",
    "#Examples:\n",
    "# 1. Breaking out words or sentences\n",
    "# 2. Separating punctuation\n",
    "# 3. Separating all hashtags in a tweet\n",
    "\n",
    "#Common library: nltk (natural language toolkit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bansal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Not required, did this since the next code block was generating an error and asked to download punkt\n",
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'there', '!']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Why tokenize?\n",
    "#Easier to map part of speech\n",
    "#Matching common words\n",
    "#Removing unwanted tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other nltk tokenizers\n",
    "#sent_tokenize: tokenize a document into sentences\n",
    "#regexp_tokenize: tokenize a string or document based on a regex pattern\n",
    "#TweetTokenizer: special class just for tweet tokenization, \n",
    "#allowing you to separate hashtags, mentions and lots of exclamation points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Let's write RegEx!\",\n",
       " \"Won't that be fun?\",\n",
       " 'I sure think so.',\n",
       " 'Can you find 4 sentences?',\n",
       " 'Or [perhaps, all] 19 words?']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "#Split my_string into sentences\n",
    "sentences = sent_tokenize(my_string)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Can', 'you', 'find', '4', 'sentences', '?']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenize the 4th sentence into words\n",
    "tokenize_sent = word_tokenize(sentences[3])\n",
    "tokenize_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!',\n",
       " \"'s\",\n",
       " ',',\n",
       " '.',\n",
       " '19',\n",
       " '4',\n",
       " '?',\n",
       " 'Can',\n",
       " 'I',\n",
       " 'Let',\n",
       " 'Or',\n",
       " 'RegEx',\n",
       " 'Wo',\n",
       " 'all',\n",
       " 'be',\n",
       " 'find',\n",
       " 'fun',\n",
       " \"n't\",\n",
       " 'perhaps',\n",
       " 'sentences',\n",
       " 'so',\n",
       " 'sure',\n",
       " 'that',\n",
       " 'think',\n",
       " 'words',\n",
       " 'write',\n",
       " 'you'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make a set of unique tokens in the entire my_string\n",
    "unique_tokens = set(word_tokenize(my_string))\n",
    "unique_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Tokenization with NLTK and regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regex groups and ranges\n",
    "# OR is represented using |\n",
    "# you can define a group using ()\n",
    "# you can define explicit character ranges using []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', 'has', '11', 'cats']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using regex find all digits and words\n",
    "re.findall(('(\\d+|\\w+)'), 'He has 11 cats.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regex ranges and groups\n",
    "# [A-Za-z]+  matches upper and lowercase English alphabet\n",
    "# [0-9] matches numbers from 0 to 9\n",
    "# [A-Za-z\\-\\.] matches upper and lowercase English alphabet, - and .  # \\ is an escape charater \n",
    "# (a-z) matches a, - and z #this will only match if there is a-z, it won't match a- etc.\n",
    "# (\\s+|,)  matches spaces or a comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 35), match='match lowercase spaces nums like 12'>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_str = 'match lowercase spaces nums like 12, but no commas'\n",
    "re.match('[a-z0-9 ]+', my_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SOLDIER',\n",
       " '#1',\n",
       " 'Found',\n",
       " 'them',\n",
       " '?',\n",
       " 'In',\n",
       " 'Mercea',\n",
       " '?',\n",
       " 'The',\n",
       " 'coconut',\n",
       " 's',\n",
       " 'tropical',\n",
       " '!']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#regexp_tokenize\n",
    "my_str = \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\"\n",
    "regexp_tokenize(my_str, r\"(\\w+|#\\d|\\?|!)\") #OR #re.findall(r\"(\\w+|#\\d|\\?|!)\",my_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This',\n",
       "  'is',\n",
       "  'the',\n",
       "  'best',\n",
       "  '#nlp',\n",
       "  'exercise',\n",
       "  'ive',\n",
       "  'found',\n",
       "  'online',\n",
       "  '!',\n",
       "  '#python'],\n",
       " ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'],\n",
       " ['Thanks', '@datacamp', ':)', '#nlp', '#python']]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TweetTokenizer\n",
    "tweets = ['This is the best #nlp exercise ive found online! #python',\n",
    " '#NLP is super fun! <3 #learning',\n",
    " 'Thanks @datacamp :) #nlp #python']\n",
    "\n",
    "#Hashtags on the first tweet\n",
    "regexp_tokenize(tweets[0], r\"#\\w+\")\n",
    "\n",
    "#Mentions and hashtags on the last tweet\n",
    "regexp_tokenize(tweets[-1], r\"[#@]\\w+\")\n",
    "\n",
    "#Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()  #create an instance of TweetTokenizer class\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]     #Use .tokenize() method of tknzr\n",
    "all_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-ascii tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wann gehen wir Pizza essen? ðŸ• Und fÃ¤hrst du mit Ãœber? ðŸš•'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#German with emoji\n",
    "german_text = 'Wann gehen wir Pizza essen? ðŸ• Und fÃ¤hrst du mit Ãœber? ðŸš•'\n",
    "german_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wann',\n",
       " 'gehen',\n",
       " 'wir',\n",
       " 'Pizza',\n",
       " 'essen',\n",
       " '?',\n",
       " 'ðŸ•',\n",
       " 'Und',\n",
       " 'fÃ¤hrst',\n",
       " 'du',\n",
       " 'mit',\n",
       " 'Ãœber',\n",
       " '?',\n",
       " 'ðŸš•']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenize words\n",
    "word_tokenize(german_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wann', 'Pizza', 'Und', 'Ãœber']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenize only capital words\n",
    "regexp_tokenize(german_text, r\"[A-ZÃœ]\\w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ðŸ•', 'ðŸš•']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenize and print only emoji\n",
    "#Unicode ranges for emoji are:\n",
    "#('\\U0001F300'-'\\U0001F5FF'), ('\\U0001F600-\\U0001F64F'), ('\\U0001F680-\\U0001F6FF'), and ('\\u2600'-\\u26FF-\\u2700-\\u27BF')\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "regexp_tokenize(german_text, emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charting word length with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting word length in a histogram\n",
    "words = word_tokenize(my_string)\n",
    "word_length = [len(w) for w in words]\n",
    "plt.hist(word_length)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting words in each line\n",
    "text = \"SCENE 1: [wind] [clop clop clop] \\nKING ARTHUR: Whoa there!  [clop clop clop] \\nSOLDIER #1: Halt!  Who goes there?\\nARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\\nSOLDIER #1: Pull the other one!\\nARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.\\nSOLDIER #1: What?  Ridden on a horse?\\nARTHUR: Yes!\\nSOLDIER #1: You're using coconuts!\\nARTHUR: What?\\nSOLDIER #1: You've got two empty halves of coconut and you're bangin' 'em together.\\nARTHUR: So?  We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--\\nSOLDIER #1: Where'd you get the coconuts?\\nARTHUR: We found them.\\nSOLDIER #1: Found them?  In Mercea?  The coconut's tropical!\\nARTHUR: What do you mean?\\nSOLDIER #1: Well, this is a temperate zone.\\nARTHUR: The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?\\nSOLDIER #1: Are you suggesting coconuts migrate?\\nARTHUR: Not at all.  They could be carried.\\nSOLDIER #1: What?  A swallow carrying a coconut?\\nARTHUR: It could grip it by the husk!\\nSOLDIER #1: It's not a question of where he grips it!  It's a simple question of weight ratios!  A five ounce bird could not carry a one pound coconut.\\nARTHUR: Well, it doesn't matter.  Will you go and tell your master that Arthur from the Court of Camelot is here.\\nSOLDIER #1: Listen.  In order to maintain air-speed velocity, a swallow needs to beat its wings forty-three times every second, right?\\nARTHUR: Please!\\nSOLDIER #1: Am I right?\\nARTHUR: I'm not interested!\\nSOLDIER #2: It could be carried by an African swallow!\\nSOLDIER #1: Oh, yeah, an African swallow maybe, but not a European swallow.  That's my point.\\nSOLDIER #2: Oh, yeah, I agree with that.\\nARTHUR: Will you ask your master if he wants to join my court at Camelot?!\"\n",
    "\n",
    "#Splitting the text into lines\n",
    "lines = re.split(r\"\\n\", text)    #text.split(\"\\n\") can also be used\n",
    "\n",
    "#Replacing lines for speaker names\n",
    "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\" # \"?\" means 0 or more times and \"{2,}\" means 2 or more times\n",
    "lines = [re.sub(pattern, '', l) for l in lines]\n",
    "\n",
    "#Tokenize each line\n",
    "tokenized_lines = [regexp_tokenize(s, r\"\\w+\") for s in lines]\n",
    "\n",
    "#Frequency list of lengths\n",
    "line_num_words = [len(tl) for tl in tokenized_lines]\n",
    "\n",
    "#Plot\n",
    "plt.hist(line_num_words)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Topic Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
