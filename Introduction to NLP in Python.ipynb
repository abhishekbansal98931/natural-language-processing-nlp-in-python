{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regex are strings with a special syntax\n",
    "#Allows us to match patterns in other strings\n",
    "#Applications of regex: Find all web links in a document, parse email addresses, remove / replace unwanted characters etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#re.match(pattern, string)\n",
    "re.match('abc', 'abcdef')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 2), match='hi'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#special patterns\n",
    "word_regex = '\\w+'\n",
    "re.match(word_regex, 'hi there!') #matches first word in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common regex patterns (there are 100s of them and these are just a few common ones)\n",
    "# \\w+  word (\\w will just pull individual characters and \\w+ will pull word)\n",
    "# \\d   digit\n",
    "# \\s   space\n",
    "# .*   wildcard (matches any letter or symbol)\n",
    "# + or * greedy match (matches repeats of sigle letter or whole patterns)\n",
    "# \\S   not space\n",
    "# []   create a group of characters eg. [a-z] lowercase group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python re module\n",
    "#re: module\n",
    "#split: split a string on regex\n",
    "#findall: find all patterns in a string\n",
    "#search: search for a pattern\n",
    "#match: match an entire string or substring based on a pattern\n",
    "\n",
    "#pass pattern first and then string\n",
    "#may return an iterator, string or match object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Split', 'on', 'spaces.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\s+', 'Split on spaces.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let', 's', 'wrtie', 'RegEx']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"\\w+\", \"Let's wrtie RegEx!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It is important to prefix regex patterns with r to ensure that patterns are interpreted correctly\n",
    "#specially escape sequences in strings\n",
    "#Eg: \"\\n\" in Python is used to indicate a new line but if we use r prefix, it will be interpreted as raw string \n",
    "#that is charater \"\\\" followed by character \"n\" and not as a new line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Let's write RegEx\",\n",
       " \"  Won't that be fun\",\n",
       " '  I sure think so',\n",
       " '  Can you find 4 sentences',\n",
       " '  Or perhaps, all 19 words',\n",
       " '']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\"\n",
    "\n",
    "#Split on Sentence endings\n",
    "re.split(r\"[.?!]\", my_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let', 'RegEx', 'Won', 'Can', 'Or']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find all capitalised words\n",
    "re.findall(r\"[A-Z]\\w+\", my_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Let's\",\n",
       " 'write',\n",
       " 'RegEx!',\n",
       " \"Won't\",\n",
       " 'that',\n",
       " 'be',\n",
       " 'fun?',\n",
       " 'I',\n",
       " 'sure',\n",
       " 'think',\n",
       " 'so.',\n",
       " 'Can',\n",
       " 'you',\n",
       " 'find',\n",
       " '4',\n",
       " 'sentences?',\n",
       " 'Or',\n",
       " 'perhaps,',\n",
       " 'all',\n",
       " '19',\n",
       " 'words?']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split string on spaces\n",
    "re.split(r\"\\s+\", my_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4', '19']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"\\d+\", my_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(2, 4), match='cd'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Difference between re.search() and re.match()\n",
    "#when the pattern that we are looking for is present in the beginning of the string then both will give identical results\n",
    "#Eg:\n",
    "re.match('abc','abcde') #will give result as 'abc'\n",
    "re.search('abc','abcde') #will give result as 'abc'\n",
    "\n",
    "#match tries to match a string from the beginning and search will go through the entire string\n",
    "#Eg:\n",
    "re.match('cd','abcde') #will not give any result\n",
    "re.search('cd','abcde') #will give result as 'cd'\n",
    "\n",
    "#use match when you want to look for pattern specifically at the beginning of the string\n",
    "#and use search when you want to look for pattern anywhere in the string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turning a string or document into tokens (smaller chunks)\n",
    "#One step in preparing a text for NLP\n",
    "#Many different theories and rules\n",
    "#You can create your own rules using regular expressions\n",
    "#Examples:\n",
    "# 1. Breaking out words or sentences\n",
    "# 2. Separating punctuation\n",
    "# 3. Separating all hashtags in a tweet\n",
    "\n",
    "#Common library: nltk (natural language toolkit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bansal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Not required, did this since the next code block was generating an error and asked to download punkt\n",
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'there', '!']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Why tokenize?\n",
    "#Easier to map part of speech\n",
    "#Matching common words\n",
    "#Removing unwanted tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other nltk tokenizers\n",
    "#sent_tokenize: tokenize a document into sentences\n",
    "#regexp_tokenize: tokenize a string or document based on a regex pattern\n",
    "#TweetTokenizer: special class just for tweet tokenization, \n",
    "#allowing you to separate hashtags, mentions and lots of exclamation points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Let's write RegEx!\",\n",
       " \"Won't that be fun?\",\n",
       " 'I sure think so.',\n",
       " 'Can you find 4 sentences?',\n",
       " 'Or perhaps, all 19 words?']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "#Split my_string into sentences\n",
    "sentences = sent_tokenize(my_string)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Can', 'you', 'find', '4', 'sentences', '?']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenize the 4th sentence into words\n",
    "tokenize_sent = word_tokenize(sentences[3])\n",
    "tokenize_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!',\n",
       " \"'s\",\n",
       " ',',\n",
       " '.',\n",
       " '19',\n",
       " '4',\n",
       " '?',\n",
       " 'Can',\n",
       " 'I',\n",
       " 'Let',\n",
       " 'Or',\n",
       " 'RegEx',\n",
       " 'Wo',\n",
       " 'all',\n",
       " 'be',\n",
       " 'find',\n",
       " 'fun',\n",
       " \"n't\",\n",
       " 'perhaps',\n",
       " 'sentences',\n",
       " 'so',\n",
       " 'sure',\n",
       " 'that',\n",
       " 'think',\n",
       " 'words',\n",
       " 'write',\n",
       " 'you'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make a set of unique tokens in the entire my_string\n",
    "unique_tokens = set(word_tokenize(my_string))\n",
    "unique_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
